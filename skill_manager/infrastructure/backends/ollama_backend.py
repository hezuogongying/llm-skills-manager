"""
Ollama æœ¬åœ°æ¨¡å‹åç«¯å®ç°
"""
import logging
from typing import List, Dict, Any, Optional

from ...core.interfaces.llm_backend import ILLMBackend, IMessage, IModelConfig

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class OllamaBackend(ILLMBackend):
    """
    Ollama æœ¬åœ°æ¨¡å‹åç«¯å®ç°

    éµå¾ªä¾èµ–å€’ç½®åŸåˆ™ - å®ç° ILLMBackend æ¥å£
    """

    DEFAULT_BASE_URL = "http://localhost:11434"

    def __init__(
        self,
        model: str = "llama3.2",
        base_url: Optional[str] = None,
        config: Optional[IModelConfig] = None
    ):
        """
        åˆå§‹åŒ– Ollama åç«¯

        Args:
            model: æ¨¡å‹åç§°ï¼ˆå¦‚ "llama3.2", "qwen2.5"ï¼‰
            base_url: Ollama æœåŠ¡åœ°å€
            config: æ¨¡å‹é…ç½®ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ model å’Œ base_url å‚æ•°ï¼‰
        """
        import requests

        # å¦‚æœæä¾›äº† configï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™åˆ›å»ºæ–°çš„
        if config:
            self.config = config
        else:
            self.config = IModelConfig(
                model=model,
                base_url=base_url or self.DEFAULT_BASE_URL
            )

        self._requests = requests
        logger.info(f"âœ… Ollama backend initialized: model={self.config.model}, base_url={self.config.base_url}")

    def complete(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """å‘é€æ¶ˆæ¯å¹¶è·å–å“åº”"""
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)

        logger.debug(f"ğŸ“¤ Sending {len(messages)} messages to Ollama ({self.config.model})")

        # æ³¨æ„ï¼šOllama çš„å·¥å…·è°ƒç”¨æ”¯æŒæœ‰é™ï¼Œtools å‚æ•°æš‚ä¸ä½¿ç”¨
        response = self._requests.post(
            f"{self.config.base_url}/api/chat",
            json={
                "model": self.config.model,
                "messages": full_messages,
                "stream": False
            },
            timeout=120
        )
        response.raise_for_status()
        result = response.json()["message"]["content"]

        logger.debug(f"ğŸ“¥ Received response from Ollama: {len(result)} characters")
        return result

    def get_model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        return f"ollama/{self.config.model}"

    def configure(self, config: IModelConfig) -> None:
        """é‡æ–°é…ç½®åç«¯"""
        self.config = config
        logger.info(f"ğŸ”„ Ollama backend reconfigured: model={config.model}")
